# d7050e_lab5

In `d7050e_lab4` we extended the compiler with a type checker performing _semantic analysis_ of the input program.

In this lab you will continue working on your compiler code base providing a simple backend translating the input program to assembly.

## Learning outcomes

- The role of backend code generation in a compiler.

- RISC like instruction sets, and typical memory regions, program/instruction, heap and stack.

- The basics of stack machines and their emulation by sequences of assembly language instructions.

- Managing state for code generation, function addresses, local store (function frame generation)

- Automated code generation for AST constructs

  - expressions

    - binary/unary operations

    - local variables

    - function calls

  - local scopes

  - conditionals

    - if then else expressions

    - while loops

- Function declarations

- Top level programs

- Optional for higher grades, includes but are not limited to:

  - Arrays

  - User defined data structures (structs enums)

  - Handling mutually recursive functions

  - Optimizations

    - Reducing redundant push/pops

    - Local variables in registers

    - Register based expression evaluation and stack spilling

---

## RISC like instruction sets

The MIPS 32-bit processor can be seen as the archetype for (all) modern RISC based architectures. The MIPS 3k instruction set is highly symmetric. Registers are general purpose, with the following exceptions:

- `zero`, always reads zero

- `ra`, hard wired to store resume address on `bal` (branch and link)

By convention we use `sp` to represent the stack pointer, and `fp` to represent the frame pointer.

For the purpose of this lab we recommend the following stack layout (resembling the MIPS calling convention):

```text
    // stack frame layout
    //
    // 16[fp]    arg 1
    // 12[fp]    arg 2
    //  8[fp]    arg 3
    //  4[fp]    ra
    //  0[fp]    old_fp
    // -4[fp]    local 1
    // -8[fp]    local 2, etc.
```

This differs from the usual MIPS ABI (calling convention) in that all arguments are passed on the stack, while the MIPS ABI uses `a0`, `a1`, `a2`, `a3` for the first 4 arguments. 


## MIPS model

The [mips](https://vesuvio-git.neteq.ltu.se/pln/mips) provides a simple virtual machine supporting a subset of the classic MIPS3k ISA (Instruction Set Architecture). 

The virtual machine implements the execution in single-cycle mode (i.e., we don't have to care about delay slots).

The crate provides builders for instructions (`addi, add` etc.) and streams of instructions `Instr`.

See crate `README.md`, for usage details. You may also look at the tests as usage examples, asserting end state properties etc.

## Code generation

In this lab you will implement a simple backend using the [mips](https://vesuvio-git.neteq.ltu.se/pln/mips) crate. Later for higher grades you may choose to either extend functionality, improve generated code performance or support some other target. To the latter you may e.g., implement alternative targets by forking the `mips` crate as a starting point. Alternatively you may interface to `llvm` by means of the low level [llvm-sys](https://crates.io/crates/llvm-sys) or some higher level abstraction build on-top of `llvm-sys`, e.g. [inkwell](https://github.com/TheDan64/inkwell).

It is recommended to (first) model code generation taking a stack machine approach. Later you may optionally improve performance of the generated code by looking into register allocation for variables, arguments and return values.

A stack machine operates by functions (and operators) `pop`:ing arguments from the stack, performing computations and returning a set of results on the stack (for our purpose it will be just one return value).

For further details on the stack machines concept, see, e.g. [Stack Machine](https://en.wikipedia.org/wiki/Stack_machine).

## Push and Pop

Using the builder patterns for instructions we can emulate the behavior of a stack machine using sequences of native MIPS instructions.

The push and pop operations may be implemented as:

```asm
addi sp, sp, -4   # allocate space on stack
sw   <reg>, 0(sp) # store the value of register <reg> on top of stack

lw   <reg>, 0(sp) # load the value from top of stack into register <reg>
addi sp, sp, 4    # deallocate space from stack
```


## Expressions

Generated code that computes an expression formed from the binary operation `1 + 2` may look something like this:

```asm
addi t0, zero, 1
push t0  # macro that pushes register t0 on top of stack
addi t0, zero, 2
push t0 
pop  t1  # macro that pops register from top of stack
pop  t0  
add  t0, t0, t1 
push t0
```

In general, an expression formed from binary operations can be generated by

- code that computes left operand
- code that computes right operand
- code that performs operation

And similarly for unary operations
- code that computes operand
- code that performs operation

Notice arguments and results are passed on the stack.

`Unit` values can be optimized out (neither allocated on stack nor code generated).

## Local variable lookup

You will need to implement an *environment* similar to the variable and type environments from previous labs. You will need to associate identifiers with offsets of the the stack frame (see above).

The generated code `fp` relative read of a variable `a` might look something like this:

```asm
lw   t0, -4(fp) # assuming `a` is the first allocated in the frame
push t0
```

## Allocations

You may dynamically allocate storage for each allocation (let binding), or first make pass over a block of statements and allocate the complete frame at once. For the latter a single instruction can be generated taking all allocations for the block into account (thus implies less overhead).

### let binding and assignments

A let binding can be seen as an allocation followed by an assignment.

Assignments are similar to local variable lookups described above, and the generated code might look like this.

```asm
pop t0
sw  t0, -4(fp) # assuming `a` is the first allocated in the frame
```

## Control flow

### If then else

If then else expressions and conditional statements can be treated in a similar fashion.

The generated code might look like this:

- code generated to compute condition (expression) to top of stack (C)
- `pop t0` (B1)
- `beq t0, zero, <offset_else>`
- code generated for `then` arm (T)
- `b   <offset_end>` (B2)
- code generated for `else` arm (E)

The code generation can be be done by first generating code for (T) and (E), then computing the `<offset_else>` and `<offset_end>` values based on the size (number of instructions) for (T+B2) and (E) respectively.

Finally emit the concatenation of (C, B1, T, B2, E). Alternatively you give unique labels to `else` and `end`, but that is likely more complex.

For `if then` statements B2 and E can be omitted.

### While

While follows the same approach as above

- code generated to compute condition (C)
- `pop t0` (B1)
- `beq t0, zero, <offset_end>`
- code generated for while body (W)
- `b   <offset_start>` (B2)

### Function calls

For a stack machine function calls are just n-ary operations:

- code to generate each argument
- `bal <fn_address>`, or `bal_label <fn_label>`

Here you might go for either keeping track of the addresses associated to each function, or come up with unique labels for each function (as functions follow local scoping rules with shadowing of outer scopes).

## Function declarations

A function declaration contains the identifier, the set of parameters, the function body and the return type (`Unit` if not stated).

We can follow the scheme of the type checker and generate code for each function only once.

During code generation we can recursively generate code for each inner (locally defined) function, keeping track of scopes for function call lookups.

The generated should follow the frame layout discussed above:

- code for function prelude 

  ```asm
    push ra            # return address`
    push fp            # old frame pointer
    addi fp, sp, zero  # 
  ```
- single allocation of local variables `<size locals>`, or deferred to later (let)
- code for function body
- code for function postlude
  ```asm
  pop t0 # return value if not `Unit` as above
  addi sp, sp, <size locals>
  pop  fp
  pop  ra
  push t0 # return value if not `Unit` as above
  jr   ra
  ```  

Notice, local scopes share the same stack frame (inner locals should be de-allocated once local scope ends), thus the same memory location can/will be re-used for representing different local variables.

For your implementation you will need to keep track of `<size locals>` for the stack frame. 

For the implementation you can add `.label` to the sequence of instructions(this will refer to the sequence entry point).

## Program

A program holds a set of top level function declarations. Generate code for each one starting with the `main` function.

## Hints

The code generation will be very similar in structure to the type checker. While the type checker evaluates to types, the code generation evaluates to generated code. Similar to the type checker each function is visited only once.

As usual start from the expressions, make sure that code is correctly generated for expressions with only literals. Proceed with local variables (here you may inject assumed environment to "fake" the frame generation).

Proceed with let bindings and assignments, if then else expressions and statements, and finally go for function declarations and function calls.

Use the logging framework to inspect the code execution and provide tests for each construct you implement. 

## Final comments

Writing a compiler from scratch is the rite of passage for any computer scientist! You should all feel proud and take a moment to reflect on the knowledge gained along this journey.

Some highlights:
- Regular expressions and regular languages.
- Parsing of context free grammars.
- Abstract syntax tree representation.
- Pretty printing thereof (or maybe not so pretty..)
- Operational semantics and natural interpretation as implemented by your VM.
- Semantic analysis as implemented by your type checker.
- Stack machines and code generation.

In the final lab (6), you will add the final touch in terms of a simple command line interface exposing various features of your RnR compiler.




